Floating point
From Wikipedia, the free encyclopedia
Jump to: navigation, search
The first programmable computer, the Z3 included floating point arithmetic (replica on display at Deutsches Museum in Munich).
A diagram showing a representation of a floating point number using a mantissa and an exponent.

In computing, floating point describes a method of representing real numbers in a way that can support a wide range of values. Numbers are, in general, represented approximately to a fixed number of significant digits and scaled using an exponent. The base for the scaling is normally 2, 10 or 16. The typical number that can be represented exactly is of the form:

    Significant digits Ã— baseexponent

The term floating point refers to the fact that the radix point (decimal point, or, more commonly in computers, binary point) can "float"; that is, it can be placed anywhere relative to the significant digits of the number. This position is indicated separately in the internal representation, and floating-point representation can thus be thought of as a computer realization of scientific notation. Over the years, a variety of floating-point representations have been used in computers. However, since the 1990s, the most commonly encountered representation is that defined by the IEEE 754 Standard.

The advantage of floating-point representation over fixed-point and integer representation is that it can support a much wider range of values. For example, a fixed-point representation that has seven decimal digits with two decimal places can represent the numbers 12345.67, 123.45, 1.23 and so on, whereas a floating-point representation (such as the IEEE 754 decimal32 format) with seven decimal digits could in addition represent 1.234567, 123456.7, 0.00001234567, 1234567000000000, and so on. The floating-point format needs slightly more storage (to encode the position of the radix point), so when stored in the same space, floating-point numbers achieve their greater range at the expense of precision.

The speed of floating-point operations, commonly referred to in performance measurements as FLOPS, is an important machine characteristic, especially in software that performs large-scale mathematical calculations.
Contents

    1 Overview
        1.1 Some other computer representations for non-integral numbers
    2 Range of floating-point numbers
    3 History
    4 IEEE 754: floating point in modern computers
        4.1 Internal representation
        4.2 Special values
            4.2.1 Signed zero
            4.2.2 Subnormal numbers
            4.2.3 Infinities
            4.2.4 NaNs
            4.2.5 IEEE 754 design rationale
    5 Representable numbers, conversion and rounding
        5.1 Rounding modes
    6 Floating-point arithmetic operations
        6.1 Addition and subtraction
        6.2 Multiplication and division
    7 Dealing with exceptional cases
    8 Accuracy problems
        8.1 Machine precision and backward error analysis
        8.2 Minimizing the effect of accuracy problems
    9 See also
    10 Notes and references
    11 Further reading
    12 External links


